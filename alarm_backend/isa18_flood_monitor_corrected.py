"""
ISA 18.2 Flood Monitor - CORRECTED VERSION

This is the corrected version that properly filters actual alarms from operator
actions and system events, providing accurate ISA 18.2 health calculations.

Key Difference from Original:
- Uses isa18_csv_reader.read_csv_alarms_only() instead of reading all events
- Filters out ~95% of records that are not actual alarms
- Provides accurate ISA health percentages

Usage:
    from isa18_flood_monitor_corrected import compute_isa18_flood_summary_corrected
    
    result = compute_isa18_flood_summary_corrected(
        folder_path=PVCI_FOLDER,
        window_minutes=10,
        threshold=10
    )
"""

from __future__ import annotations

import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timezone

# Import the corrected CSV reader
from isa18_csv_reader import read_csv_alarms_only, ISA18Config

# Import base functions from original module
from isa18_flood_monitor import (
    _list_csv_files,
    _parse_user_iso,
    _to_utc_iso,
    # We'll reimplement the core logic using filtered data
)

logger = logging.getLogger(__name__)


def _read_alarm_rows_for_file(file_path: str, config: ISA18Config = None) -> 'pd.DataFrame':
    """
    Read ONLY actual alarm rows from a CSV file (ISA 18.2 compliant).
    
    This replaces _read_rows_for_file from the original module.
    Returns dataframe with Event Time, Location Tag, Source, Condition columns.
    """
    import pandas as pd
    
    if config is None:
        config = ISA18Config()
    
    # Read with all columns to preserve Location Tag and Condition
    df = read_csv_alarms_only(file_path, config=config, return_all_columns=True)
    
    if df.empty:
        return pd.DataFrame(columns=['Event Time', 'Location Tag', 'Source', 'Condition'])
    
    # Ensure required columns
    for col in ('Event Time', 'Location Tag', 'Source', 'Condition'):
        if col not in df.columns:
            df[col] = ''
    
    # Select and normalize
    out = df[['Event Time', 'Location Tag', 'Source', 'Condition']].copy()
    out['Event Time'] = pd.to_datetime(out['Event Time'], errors='coerce', utc=True)
    out = out.dropna(subset=['Event Time'])
    
    for c in ('Location Tag', 'Source', 'Condition'):
        out[c] = out[c].astype(str)
    
    return out


def compute_isa18_flood_summary_corrected(
    folder_path: str,
    window_minutes: int = 10,
    threshold: int = 10,
    operator_map: Optional[Dict[str, Any]] = None,
    start_time: Optional[str] = None,
    end_time: Optional[str] = None,
    include_records: bool = False,
    include_windows: bool = False,
    include_alarm_details: bool = False,
    top_n: int = 10,
    max_windows: Optional[int] = 10,
    events_sample: bool = False,
    events_sample_max: int = 0,
    isa_config: Optional[ISA18Config] = None,
) -> Dict[str, Any]:
    """
    Compute ISA 18.2 flood summary using CORRECTED alarm filtering.
    
    This function uses read_csv_alarms_only() to filter out operator actions
    and system events, providing accurate ISA 18.2 calculations.
    
    Returns same structure as original compute_isa18_flood_summary but with
    corrected alarm counts and health percentages.
    """
    import pandas as pd
    from datetime import timedelta
    from collections import defaultdict, deque
    
    if isa_config is None:
        isa_config = ISA18Config()
    
    logger.info("Computing ISA 18.2 flood summary (CORRECTED - alarms only)")
    
    try:
        files = _list_csv_files(folder_path)
        if not files:
            logger.warning(f"No CSV files found in {folder_path}")
            return _empty_summary()
        
        # Parse time filters
        start_dt = _parse_user_iso(start_time)
        end_dt = _parse_user_iso(end_time)
        
        # Read all alarm data (filtered)
        all_alarms = []
        for fp in files:
            df = _read_alarm_rows_for_file(fp, config=isa_config)
            if df is not None and not df.empty:
                # Apply time filter
                if start_dt or end_dt:
                    ts = df['Event Time']
                    mask = pd.Series([True] * len(df))
                    if start_dt:
                        mask &= (ts >= start_dt)
                    if end_dt:
                        mask &= (ts <= end_dt)
                    df = df[mask]
                
                if not df.empty:
                    all_alarms.append(df)
        
        if not all_alarms:
            logger.warning("No alarm data found after filtering")
            return _empty_summary()
        
        # Combine all alarms
        df_all = pd.concat(all_alarms, ignore_index=True)
        df_all = df_all.sort_values('Event Time').reset_index(drop=True)
        
        total_alarms = len(df_all)
        logger.info(f"Total ACTUAL alarms (after filtering): {total_alarms:,}")
        
        # Calculate observation period
        min_time = df_all['Event Time'].min()
        max_time = df_all['Event Time'].max()
        observation_duration = (max_time - min_time).total_seconds() / 60.0  # minutes
        
        # Detect flood windows using sliding window
        flood_windows = []
        window_size = timedelta(minutes=window_minutes)
        
        # Simple sliding window flood detection
        df_sorted = df_all.sort_values('Event Time')
        window_queue = deque()
        
        for idx, row in df_sorted.iterrows():
            ts = row['Event Time']
            window_queue.append(ts)
            
            # Remove old timestamps outside window
            while window_queue and (ts - window_queue[0]) > window_size:
                window_queue.popleft()
            
            # Check if flood condition met
            if len(window_queue) >= threshold:
                # Record flood window
                window_start = window_queue[0]
                window_end = ts
                flood_windows.append({
                    'start': window_start,
                    'end': window_end,
                    'count': len(window_queue),
                    'peak_time': ts
                })
        
        # Merge overlapping/consecutive flood windows
        if flood_windows:
            merged_windows = []
            current = flood_windows[0]
            
            for next_window in flood_windows[1:]:
                # If windows overlap or are within 1 minute, merge
                if (next_window['start'] - current['end']).total_seconds() <= 60:
                    current['end'] = max(current['end'], next_window['end'])
                    current['count'] = max(current['count'], next_window['count'])
                else:
                    merged_windows.append(current)
                    current = next_window
            
            merged_windows.append(current)
            flood_windows = merged_windows
        
        # Calculate flood duration
        flood_duration_min = sum([
            (w['end'] - w['start']).total_seconds() / 60.0 
            for w in flood_windows
        ])
        
        # ISA Health calculations
        percent_time_in_flood = (flood_duration_min / observation_duration * 100) if observation_duration > 0 else 0
        isa_health_pct = 100 - percent_time_in_flood
        
        # Find peak window
        peak_window = max(flood_windows, key=lambda w: w['count']) if flood_windows else None
        
        # Daily breakdown
        df_all['date'] = df_all['Event Time'].dt.date
        daily_stats = []
        
        for date in df_all['date'].unique():
            day_df = df_all[df_all['date'] == date]
            day_start = pd.Timestamp(date, tz='UTC')
            day_end = day_start + timedelta(days=1)
            
            # Find flood windows for this day
            day_floods = [w for w in flood_windows 
                          if w['start'] >= day_start and w['start'] < day_end]
            
            day_flood_duration = sum([
                (w['end'] - w['start']).total_seconds() / 60.0 
                for w in day_floods
            ])
            
            day_total_min = 1440  # minutes in a day
            day_percent_flood = (day_flood_duration / day_total_min * 100)
            
            daily_stats.append({
                'date': str(date),
                'flood_duration_min': day_flood_duration,
                'percent_time_in_flood': day_percent_flood,
                'isa_health_pct': 100 - day_percent_flood,
                'peak_10min_count': max([w['count'] for w in day_floods]) if day_floods else 0,
            })
        
        # Build result
        result = {
            'plant_folder': folder_path,
            'generated_at': datetime.now(timezone.utc).isoformat(),
            'params': {
                'window_minutes': window_minutes,
                'threshold': threshold,
                'start_time': start_time,
                'end_time': end_time,
                'isa_18_2_corrected': True,  # Mark as corrected version
                'alarm_filtering_enabled': True,
            },
            'overall': {
                'total_observation_duration_min': observation_duration,
                'total_alarms': total_alarms,
                'flood_windows_count': len(flood_windows),
                'flood_duration_min': flood_duration_min,
                'percent_time_in_flood': percent_time_in_flood,
                'isa_overall_health_pct': isa_health_pct,
                'peak_10min_count': peak_window['count'] if peak_window else 0,
                'peak_10min_window_start': _to_utc_iso(peak_window['start']) if peak_window else None,
                'peak_10min_window_end': _to_utc_iso(peak_window['end']) if peak_window else None,
                'compliance': {
                    'target': '<1% time in flood',
                    'value': percent_time_in_flood,
                    'meets': percent_time_in_flood < 1.0,
                },
            },
            'by_day': sorted(daily_stats, key=lambda x: x['date']),
        }
        
        # Add records if requested (for Top Flood Windows frontend chart)
        if include_records and flood_windows:
            records = []
            # Sort windows by count (descending) and take top windows
            sorted_windows = sorted(flood_windows, key=lambda w: w['count'], reverse=True)
            if max_windows and max_windows > 0:
                sorted_windows = sorted_windows[:max_windows]
            
            for window in sorted_windows:
                # Get alarms within this window
                window_alarms = df_all[
                    (df_all['Event Time'] >= window['start']) & 
                    (df_all['Event Time'] <= window['end'])
                ]
                
                # Count by source
                source_counts = window_alarms['Source'].value_counts()
                top_sources = [
                    {'source': str(src), 'count': int(cnt)}
                    for src, cnt in source_counts.head(top_n).items()
                ]
                
                # Build record
                record = {
                    'peak_window_start': _to_utc_iso(window['start']),
                    'peak_window_end': _to_utc_iso(window['end']),
                    'peak_10min_count': window['count'],
                    'peak_window_details': {
                        'top_sources': top_sources,
                    },
                    'top_sources': top_sources,  # Also at top level for compatibility
                }
                records.append(record)
            
            result['records'] = records
            logger.info(f"Added {len(records)} records for frontend")
        
        logger.info(f"ISA Health (CORRECTED): {isa_health_pct:.2f}%")
        logger.info(f"Time in flood: {percent_time_in_flood:.2f}%")
        
        return result
        
    except Exception as e:
        logger.error(f"Error computing corrected ISA summary: {e}", exc_info=True)
        return _empty_summary()


def _empty_summary() -> Dict[str, Any]:
    """Return empty summary structure."""
    return {
        'plant_folder': '',
        'generated_at': datetime.now(timezone.utc).isoformat(),
        'overall': {
            'total_observation_duration_min': 0,
            'total_alarms': 0,
            'flood_windows_count': 0,
            'flood_duration_min': 0,
            'percent_time_in_flood': 0,
            'isa_overall_health_pct': 100,
        },
        'by_day': [],
    }


if __name__ == '__main__':
    """Test the corrected ISA flood monitor."""
    import json
    from config import PVCI_FOLDER
    
    logging.basicConfig(level=logging.INFO)
    
    print("\n" + "="*80)
    print("Testing ISA 18.2 Flood Monitor - CORRECTED VERSION")
    print("="*80)
    
    result = compute_isa18_flood_summary_corrected(
        folder_path=PVCI_FOLDER,
        window_minutes=10,
        threshold=10,
        include_records=False,
        include_windows=False,
        max_windows=100,
    )
    
    print("\nüìä CORRECTED ISA 18.2 Results:")
    print(f"   Total ACTUAL alarms: {result['overall']['total_alarms']:,}")
    print(f"   Flood windows: {result['overall']['flood_windows_count']}")
    print(f"   ISA Health: {result['overall']['isa_overall_health_pct']:.2f}%")
    print(f"   Time in flood: {result['overall']['percent_time_in_flood']:.2f}%")
    print(f"   Compliance: {'‚úÖ MEETS' if result['overall']['compliance']['meets'] else '‚ùå FAILS'}")
    
    # Save sample
    output_path = 'PVCI-overall-health/isa18-flood-summary-CORRECTED-sample.json'
    with open(output_path, 'w') as f:
        json.dump(result, f, indent=2)
    
    print(f"\nüíæ Sample saved to: {output_path}")
    print("\n‚úÖ Corrected ISA monitor is working!")
